%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
%
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com)
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}
\usepackage{amsmath} % Needed for align environment
\usepackage{enumitem} % For compact lists
\usepackage{listings} % For code examples
\usepackage{booktabs} % For better tables
\usepackage{hyperref} % For hyperlinks


%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report}
%\Archive{Final report}

% Article title
\PaperTitle{Integrating Structured Knowledge into Large Language Models}

% Authors (student competitors) and their info
\Authors{Konstantin Bojchevski, Matic Conradi, and Matjaž Kumin}

% Advisors
\affiliation{\textit{Advisor: Slavko Žitnik}}

% Keywords
\Keywords{LLMs, Knowledge Graphs, Wikidata, RAG, Finetuning, LoRA, Reasoning}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
This project explores integrating structured knowledge into Large Language Models (LLMs) to improve reasoning. We focus on incorporating knowledge graphs (KGs), particularly focusing on a subset of Wikidata's data about the Slovenian counties, cities, castles, rivers and mountain peaks, to enhance question answering. We explore direct input augmentation, model finetuning and LoRA based model augmentation. This report presents our approaches and methods, as well as evaluation of their impact on question answering performance.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom

% Print the title and abstract box
\maketitle

% Removes page numbering from the first page
\thispagestyle{empty}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
Large Language Models (LLMs) have significantly advanced natural language understanding and generation. However, these models often struggle with complex reasoning tasks that require structured knowledge. This project explores techniques for integrating knowledge graphs (KGs) into LLMs to enhance their ability to accurately answer complex questions.

Our focus is on incorporating structured data about the Slovenian counties, cities, castles, rivers and mountain peaks into LLMs via KGs. By leveraging semantic relationships between aforementioned entities, we aim to improve the model's performance in accuracy and reasoning. This project will evaluate different integration techniques and their impact.

\section*{Related Work}
Several previous studies have explored how to combine knowledge graphs and LLMs to improve reasoning and language understanding. Key works include:

\begin{itemize}[noitemsep]
    \item \textbf{GraphGPT: Graph Instruction Tuning for Large Language Models} \cite{Tang2024GraphGPT}: Tang et al. (2024) explored tuning LLMs with graph-structured data, demonstrating the benefits of structured knowledge in language generation.
    \item \textbf{Combining Knowledge Graphs and Large Language Models} \cite{Kau2024CombiningKGLLM}: Kau et al. (2024) investigated hybrid approaches for combining KGs and LLMs.
    \item \textbf{Deep Bidirectional Language-Knowledge Graph Pretraining} \cite{Yasunaga2022DBLP}: Yasunaga et al. (2022) introduced pretraining methods integrating bidirectional knowledge graphs into LLMs.
    \item \textbf{Graph Language Models} \cite{Plenz2024GLM}: Plenz and Frank (2024) overview the use of graph-structured information with LLMs.
    \item \textbf{Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs} \cite{Zhang2024CoK}: Zhang et al. (2024) explored integrating knowledge reasoning into LLMs by learning from knowledge graphs.
    \item \textbf{InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration} \cite{Wang2024InfuserKI}: Wang et al. (2024) proposed enhancing LLMs with KGs via infuser-guided knowledge integration.
\end{itemize}

\section*{Methodology}

\subsection*{Dataset preparation}

Our methodology for data preparation and knowledge graph construction began with programmatic data acquisition from Wikidata. A proprietary client was implemented to run our SPARQL queries to retrieve information related to Slovenian municipalities, mountain peaks, castles, their properties, and their relationships. These queries targeted specific attributes such as population counts, geographical area, elevation, and cultural heritage classifications.

The client was engineered to parse the structured JSON responses from the Wikidata Query Service, performing necessary data type conversions and managing absent data points, ultimately organizing the information into a directed graph. Entities such as municipalities, regions, peaks and castles, along with their respective attributes (e.g., population, area, elevation, heritage status), were modeled as nodes within this graph. In particular, numerical attributes were also represented as distinct nodes, including nodes for their approximate rounded values to support more flexible querying. The relationships between these entities, for instance, "is located in," "has population," or "belongs to heritage," were encoded as directed edges. This meticulously constructed knowledge graph provides a structured representation of the domain, enabling the subsequent generation of question-answer pairs through a system that utilizes predefined templates to interrogate the graph's relational structure and content.

\subsection*{RAG}

Our approach to Retrieval Augmented Generation (RAG) centers on using a structured knowledge graph to improve how LMMs answer questions. The process can be outlined as follows:

First, we establish a knowledge base in the form of a graph, following the procedure described in the previous section. From this graph, we automatically create question-answer pairs. Importantly, each of these pairs also includes a set of relevant facts, drawn directly from the graph, which provide the necessary context for each question.

Next, we evaluate the RAG system by giving these questions to an LLM in two different ways. In the first scenario, the LLM answers the questions using only its internal knowledge. In the second, the LLM receives the same questions, but this time, its input is enriched with the contextual facts we extracted earlier from our knowledge graph. We then check how accurate the LLM's answers are in both situations by comparing them to the known correct answers. This checking process is handled in two steps. Firstly, a separate LLM, which acts as an unbiased judge to see if the generated answer matches the correct one, evaluates the answers. Then answers are evaluated manually to double check everything.

Lastly, we gauge the improvement by comparing the LLM's accuracy when it answers with the extra context versus without it. This comparison helps us measure how much the retrieved facts from the knowledge graph actually help the LLM, showing how well our RAG strategy works.

\subsection*{Finetuning}

To improve the ability of a pretrained language model to reason over structured information specific to our domain, we attempted supervised finetuning on the \texttt{cjvt/GaMS-9B} model. This large Slovenian causal language model was selected due to its linguistic compatibility and public availability.

We constructed a dataset of 10{,}000 examples in the form of instruction pairs, where each sample consists of a natural language question (prompt) and a corresponding structured answer (response), typically derived from our domain-specific knowledge graph. Each example was serialized in JSON format and tokenized using the GaMS tokenizer, with a maximum sequence length of 256 tokens.

Finetuning was implemented using HuggingFace’s \texttt{Trainer} API with half-precision floating point (\texttt{fp16}) and gradient accumulation in an attempt to manage the substantial memory requirements of the 9B-parameter model. Despite careful tuning, we encountered persistent CUDA out-of-memory and initialization errors across multiple configurations (including reduced batch sizes and single/multi-GPU variants). These issues made it unable to successfully complete the finetuning process within the available hardware constraints.

While this component of the pipeline could not be finalized, we include it in the report to demonstrate the feasibility of the approach and to document the encountered limitations. In theory, finetuning offers the model a chance to learn domain-specific patterns and phrasing present in our generated graph-based examples. In contrast to RAG, which augments retrieval at inference time, finetuning aims to internalize this reasoning capability and is expected to generalize better on structured queries grounded in our knowledge graph.

\section*{Results}

\subsection*{RAG}

The integration of a knowledge graph into our Retrieval Augmented Generation (RAG) system demonstrably enhanced performance by providing a structured and contextually rich source for information retrieval. As a result, the relevance and factuality of the generated outputs saw a significant increase, generally achieving a near perfect score of 100\% in accuracy, though we should note that the validation set is quite small, only on the order of hundreds of examples, due to the fact that evaluation has been done manually in the second step of our evaluation procedure. A perfect score is a massive jump compared to sub 10\% accuracy of the model on its own.

Furthermore, the knowledge graph enabled more sophisticated reasoning capabilities, allowing the RAG model to synthesize information from multiple, interconnected data points to answer complex queries that previously yielded incomplete or superficial answers. This was particularly evident in scenarios requiring an understanding of multi-hop relationships or hierarchical structures between entities provided in the prompt. Initial observations therefore point towards a marked improvement in overall answer quality and correctness, indicated by our results.

\bibliographystyle{unsrt}
\bibliography{report}

\end{document}
