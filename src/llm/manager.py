import string
import re
from typeguard import typechecked
import torch
from transformers import pipeline

# Basic Slovene stopwords (expand as needed)
SLOVENE_STOPWORDS = set([
	"in", "je", "se", "na", "za", "v", "pa", "ki", "so", "kot", "iz", "z",
	"pri", "do", "ob", "po", "med", "nad", "pod", "pred", "skozi", "čez",
	"ali", "ter", "tudi", "še", "že", "bo", "bi", "bil", "bila", "bili",
	"bilo", "sem", "si", "sva", "sta", "smo", "ste", "jaz", "ti", "on",
	"ona", "ono", "mi", "vi", "oni", "one", "onadva", "midva", "vidva",
	"kaj", "kdo", "kje", "kdaj", "kako", "zakaj", "koliko", "kateri",
	"katera", "katero", "čigar", "čigava", "čigavo", "ta", "to", "ti",
	"te", "tem", "tej", "tega", "teh", "temu", "tej", "tema", "temi",
	"svoj", "svoja", "svoje"
])

@typechecked
class LLMManager:
	"""
	Manages interactions with a Hugging Face language model,
	maintains conversation history, and provides evaluation capabilities.
	"""
	def __init__(self, model_id: str = "cjvt/GaMS-1B-Chat"):
		"""
		Initializes the LLMManager.

		Args:
			model_id (str): The Hugging Face model ID to load.
		"""
		self.model_id = model_id
		self.__history = [] # Stores conversation history [{role: 'user', content: '...'}, {role: 'assistant', content: '...'}]
		self.llm_pipeline = None
		self._load_model()

	def _load_model(self):
		"""Loads the tokenizer and model pipeline."""
		if torch.backends.mps.is_available():
			device = torch.device("mps")
		elif torch.cuda.is_available():
			device = torch.device("cuda")
		else:
			device = torch.device("cpu")

		# For chat models, using the pipeline is often easier as it handles formatting.
		# Trust remote code if necessary for this specific model architecture
		self.llm_pipeline = pipeline(
			"text-generation",
			model=self.model_id,
			torch_dtype=torch.bfloat16 if device.type != 'mps' else torch.float16, # bfloat16 often faster, but check compatibility. float16 for MPS.
			device=device, # Explicitly set device
		)

	def _preprocess_text(self, text: str) -> set:
		"""
		Basic text preprocessing for evaluation: lowercase, remove punctuation,
		tokenize, remove stopwords.

		Args:
			text (str): The text to preprocess.

		Returns:
			set: A set of unique, relevant tokens from the text.
		"""
		text = text.lower()
		# Remove punctuation more thoroughly
		text = text.translate(str.maketrans('', '', string.punctuation))
		# Simple whitespace tokenization
		tokens = text.split()
		# Remove stopwords and short tokens (optional)
		processed_tokens = {
			token for token in tokens
			if token not in SLOVENE_STOPWORDS and len(token) > 1
		}
		return processed_tokens

	def ask(self, prompt: str) -> str:
		"""
		Sends a prompt to the LLM and gets a response.

		Args:
			prompt (str): The user's input prompt.

		Returns:
			str: The LLM's generated response, or an error message.
		"""

		current_interaction = [{"role": "user", "content": prompt}]
		messages = self.__history + current_interaction

		response = self.llm_pipeline(messages, max_new_tokens=500)
		generated_text = response[0]['generated_text']
		assistant_response = generated_text[-1]['content']

		self.__history.append({"role": "user", "content": prompt})
		self.__history.append({"role": "assistant", "content": assistant_response})

		return assistant_response

	@property
	def history(self) -> list:
		"""Returns the current conversation history."""
		return self.__history.copy() # Return a copy to prevent external modification

	def clear_history(self):
		"""Clears the conversation history."""
		self.__history = []

	def evaluate_answer_f1(self, generated_answer: str, ground_truth_answer: str) -> float:
		"""
		Evaluates the generated answer against the ground truth using F1 score
		based on overlapping non-stopword tokens.

		Args:
			generated_answer (str): The answer generated by the LLM.
			ground_truth_answer (str): The correct answer.

		Returns:
			float: The F1 score (0.0 to 1.0).
		"""
		# Preprocess both answers to get sets of relevant tokens
		generated_tokens = self._preprocess_text(generated_answer)
		ground_truth_tokens = self._preprocess_text(ground_truth_answer)

		# Handle empty sets to avoid division by zero
		if not generated_tokens or not ground_truth_tokens:
			return 0.0

		# Calculate intersection (common tokens)
		common_tokens = generated_tokens.intersection(ground_truth_tokens)

		# Calculate Precision, Recall, and F1
		precision = len(common_tokens) / len(generated_tokens)
		recall = len(common_tokens) / len(ground_truth_tokens)

		if precision + recall == 0:
			f1_score = 0.0
		else:
			f1_score = 2 * (precision * recall) / (precision + recall)

		return f1_score

if __name__ == "__main__":
	llm = LLMManager(model_id="cjvt/GaMS-1B-Chat")

	# First interaction
	prompt1 = "Kdo je France Prešeren?"
	print(f"User: {prompt1}")
	response1 = llm.ask(prompt1)
	print(f"LLM: {response1}")

	# Second interaction (model remembers the context via history)
	prompt2 = "Kje je bil rojen?"
	print(f"User: {prompt2}")
	response2 = llm.ask(prompt2)
	print(f"LLM: {response2}")